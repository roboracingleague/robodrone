{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from functools import partial\n",
    "import json\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_func_from_saved_model(saved_model_dir):\n",
    "  saved_model_loaded = tf.saved_model.load(\n",
    "      saved_model_dir, tags=[tag_constants.SERVING])\n",
    "  graph_func = saved_model_loaded.signatures[\n",
    "      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "  return graph_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_func(input_saved_model_dir,\n",
    "                   data_dir,\n",
    "                   calib_data_dir,\n",
    "                   annotation_path,\n",
    "                   input_size,\n",
    "                   output_saved_model_dir=None,\n",
    "                   conversion_params=trt.DEFAULT_TRT_CONVERSION_PARAMS,\n",
    "                   use_trt=False,\n",
    "                   num_calib_inputs=None,\n",
    "                   use_synthetic=False,\n",
    "                   batch_size=None,\n",
    "                   optimize_offline=False):\n",
    "  \"\"\"Retreives a frozen SavedModel and applies TF-TRT\n",
    "  use_trt: bool, if true use TensorRT\n",
    "  precision: str, floating point precision (FP32, FP16, or INT8)\n",
    "  batch_size: int, batch size for TensorRT optimizations\n",
    "  returns: TF function that is ready to run for inference\n",
    "  \"\"\"\n",
    "  start_time = time.time()\n",
    "  graph_func = get_func_from_saved_model(input_saved_model_dir)\n",
    "  if use_trt:\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "        input_saved_model_dir=input_saved_model_dir,\n",
    "        conversion_params=conversion_params,\n",
    "    )\n",
    "    def input_fn(input_data_dir, num_iterations):\n",
    "      dataset, image_ids = get_dataset(\n",
    "          images_dir=input_data_dir,\n",
    "          annotation_path=annotation_path,\n",
    "          batch_size=batch_size,\n",
    "          use_synthetic=False,\n",
    "          input_size=input_size)\n",
    "      for i, batch_images in enumerate(dataset):\n",
    "        if i >= num_iterations:\n",
    "          break\n",
    "        yield (batch_images,)\n",
    "        print(\"  step %d/%d\" % (i+1, num_iterations))\n",
    "        i += 1\n",
    "    if conversion_params.precision_mode != 'INT8':\n",
    "      print('Graph conversion...')\n",
    "      converter.convert()\n",
    "      if optimize_offline:\n",
    "        print('Building TensorRT engines...')\n",
    "        converter.build(input_fn=partial(input_fn, data_dir, 1))\n",
    "      converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "      graph_func = get_func_from_saved_model(output_saved_model_dir)\n",
    "    else:\n",
    "      print('Graph conversion and INT8 calibration...')\n",
    "      converter.convert(calibration_input_fn=partial(\n",
    "          input_fn, calib_data_dir, num_calib_inputs//batch_size))\n",
    "      if optimize_offline:\n",
    "        print('Building TensorRT engines...')\n",
    "        converter.build(input_fn=partial(input_fn, data_dir, 1))\n",
    "      converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "      graph_func = get_func_from_saved_model(output_saved_model_dir)\n",
    "  return graph_func, {'conversion': time.time() - start_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(graph_func,\n",
    "                  data_dir,\n",
    "                  annotation_path,\n",
    "                  batch_size,\n",
    "                  input_size,\n",
    "                  num_iterations,\n",
    "                  num_warmup_iterations,\n",
    "                  use_synthetic,\n",
    "                  display_every=100,\n",
    "                  mode='validation',\n",
    "                  target_duration=None):\n",
    "  \"\"\"Run the given graph_func on the data files provided. In validation mode,\n",
    "  it consumes TFRecords with labels and reports accuracy. In benchmark mode, it\n",
    "  times inference on real data (.jpgs).\n",
    "  \"\"\"\n",
    "  results = {}\n",
    "  predictions = {}\n",
    "  iter_times = []\n",
    "  initial_time = time.time()\n",
    "\n",
    "  dataset, image_ids = get_dataset(images_dir=data_dir,\n",
    "                        annotation_path=annotation_path,\n",
    "                        batch_size=batch_size,\n",
    "                        use_synthetic=use_synthetic,\n",
    "                        input_size=input_size)\n",
    "  if mode == 'validation':\n",
    "    for i, batch_images in enumerate(dataset):\n",
    "      start_time = time.time()\n",
    "      batch_preds = graph_func(batch_images)\n",
    "      end_time = time.time()\n",
    "      iter_times.append(end_time - start_time)\n",
    "      for key in batch_preds.keys():\n",
    "        if key not in predictions:\n",
    "          predictions[key] = [batch_preds[key]]\n",
    "        else:\n",
    "          predictions[key].append(batch_preds[key])\n",
    "      if i % display_every == 0:\n",
    "        print(\"  step %d/%d, iter_time(ms)=%.0f\" %\n",
    "              (i+1, 4096//batch_size, iter_times[-1]*1000))\n",
    "      if i > 1 and target_duration is not None and \\\n",
    "        time.time() - initial_time > target_duration:\n",
    "        break\n",
    "  elif mode == 'benchmark':\n",
    "    for i, batch_images in enumerate(dataset):\n",
    "      if i >= num_warmup_iterations:\n",
    "        start_time = time.time()\n",
    "        batch_preds = list(graph_func(batch_images).values())[0].numpy()\n",
    "        iter_times.append(time.time() - start_time)\n",
    "        if i % display_every == 0:\n",
    "          print(\"  step %d/%d, iter_time(ms)=%.0f\" %\n",
    "                (i+1, num_iterations, iter_times[-1]*1000))\n",
    "      else:\n",
    "        batch_preds = list(graph_func(batch_images).values())[0].numpy()\n",
    "      if i > 0 and target_duration is not None and \\\n",
    "        time.time() - initial_time > target_duration:\n",
    "        break\n",
    "      if num_iterations is not None and i >= num_iterations:\n",
    "        break\n",
    "\n",
    "  if not iter_times:\n",
    "    return results\n",
    "  iter_times = np.array(iter_times)\n",
    "  iter_times = iter_times[num_warmup_iterations:]\n",
    "  results['total_time'] = np.sum(iter_times)\n",
    "  results['images_per_sec'] = np.mean(batch_size / iter_times)\n",
    "  results['99th_percentile'] = np.percentile(\n",
    "      iter_times, q=99, interpolation='lower') * 1000\n",
    "  results['latency_mean'] = np.mean(iter_times) * 1000\n",
    "  results['latency_median'] = np.median(iter_times) * 1000\n",
    "  results['latency_min'] = np.min(iter_times) * 1000\n",
    "  return results, predictions, image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(predictions, image_ids, annotation_path):\n",
    "  name_map = {\n",
    "      'output_0':'boxes',\n",
    "      'output_1':'classes',\n",
    "      'output_2':'num_detections',\n",
    "      'output_3':'scores',\n",
    "  }\n",
    "  for old_key in list(predictions.keys()):\n",
    "    if old_key in name_map:\n",
    "      new_key = name_map[old_key]\n",
    "      predictions[new_key] = predictions[old_key]\n",
    "      del predictions[old_key]\n",
    "  for key in predictions:\n",
    "    predictions[key] = [t.numpy() for t in predictions[key]]\n",
    "    predictions[key] = np.vstack(predictions[key])\n",
    "    if key == 'num_detections':\n",
    "      predictions[key] = predictions[key].ravel()\n",
    "    \n",
    "  coco = COCO(annotation_file=annotation_path)\n",
    "  coco_detections = []\n",
    "  for i, image_id in enumerate(image_ids):\n",
    "    coco_img = coco.imgs[image_id]\n",
    "    image_width = coco_img['width']\n",
    "    image_height = coco_img['height']\n",
    "\n",
    "    for j in range(int(predictions['num_detections'][i])):\n",
    "      bbox = predictions['boxes'][i][j]\n",
    "      y1, x1, y2, x2 = list(bbox)\n",
    "      bbox_coco_fmt = [\n",
    "        x1 * image_width,  # x0\n",
    "        y1 * image_height,  # x1\n",
    "        (x2 - x1) * image_width,  # width\n",
    "        (y2 - y1) * image_height,  # height\n",
    "      ]\n",
    "      coco_detection = {\n",
    "        'image_id': image_id,\n",
    "        'category_id': int(predictions['classes'][i][j]),\n",
    "        'bbox': [int(coord) for coord in bbox_coco_fmt],\n",
    "        'score': float(predictions['scores'][i][j])\n",
    "      }\n",
    "      coco_detections.append(coco_detection)\n",
    "  # write coco detections to file\n",
    "  tmp_dir = 'tmp_detection_results'\n",
    "  subprocess.call(['mkdir', '-p', tmp_dir])\n",
    "  coco_detections_path = os.path.join(tmp_dir, 'coco_detections.json')\n",
    "  with open(coco_detections_path, 'w') as f:\n",
    "    json.dump(coco_detections, f)\n",
    "  cocoDt = coco.loadRes(coco_detections_path)\n",
    "  subprocess.call(['rm', '-r', tmp_dir])\n",
    "\n",
    "  # compute coco metrics\n",
    "  eval = COCOeval(coco, cocoDt, 'bbox')\n",
    "  eval.params.imgIds = image_ids\n",
    "\n",
    "  eval.evaluate()\n",
    "  eval.accumulate()\n",
    "  eval.summarize()\n",
    "\n",
    "  return eval.stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_gpu_memory(gpu_mem_cap):\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if not gpus:\n",
    "    return\n",
    "  print('Found the following GPUs:')\n",
    "  for gpu in gpus:\n",
    "    print('  ', gpu)\n",
    "  for gpu in gpus:\n",
    "    try:\n",
    "      if not gpu_mem_cap:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      else:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpu,\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(\n",
    "                memory_limit=gpu_mem_cap)])\n",
    "    except RuntimeError as e:\n",
    "      print('Can not set GPU memory config', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trt_conversion_params(max_workspace_size_bytes,\n",
    "                              precision_mode,\n",
    "                              minimum_segment_size,\n",
    "                              max_batch_size):\n",
    "  conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS\n",
    "  conversion_params = conversion_params._replace(\n",
    "      max_workspace_size_bytes=max_workspace_size_bytes)\n",
    "  conversion_params = conversion_params._replace(precision_mode=precision_mode)\n",
    "  conversion_params = conversion_params._replace(\n",
    "      minimum_segment_size=minimum_segment_size)\n",
    "  conversion_params = conversion_params._replace(\n",
    "      use_calibration=precision_mode == 'INT8')\n",
    "  conversion_params = conversion_params._replace(\n",
    "      max_batch_size=max_batch_size)\n",
    "  return conversion_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "\n",
    "  parser = argparse.ArgumentParser(description='Evaluate model')\n",
    "  parser.add_argument('--input_saved_model_dir', type=str, default=None,\n",
    "                      help='Directory containing the input saved model.')\n",
    "  parser.add_argument('--output_saved_model_dir', type=str, default=None,\n",
    "                      help='Directory in which the converted model is saved')\n",
    "  parser.add_argument('--input_size', type=int, default=640,\n",
    "                      help='Size of input images expected by the model')\n",
    "  parser.add_argument('--data_dir', type=str, default=None,\n",
    "                      help='Directory containing validation set'\n",
    "                      'TFRecord files.')\n",
    "  parser.add_argument('--annotation_path', type=str,\n",
    "                      help='Path that contains COCO annotations')\n",
    "  parser.add_argument('--calib_data_dir', type=str,\n",
    "                      help='Directory containing TFRecord files for'\n",
    "                      'calibrating INT8.')\n",
    "  parser.add_argument('--use_trt', action='store_true',\n",
    "                      help='If set, the graph will be converted to a'\n",
    "                      'TensorRT graph.')\n",
    "  parser.add_argument('--optimize_offline', action='store_true',\n",
    "                      help='If set, TensorRT engines are built'\n",
    "                      'before runtime.')\n",
    "  parser.add_argument('--precision', type=str,\n",
    "                      choices=['FP32', 'FP16', 'INT8'], default='FP32',\n",
    "                      help='Precision mode to use. FP16 and INT8 only'\n",
    "                      'work in conjunction with --use_trt')\n",
    "  parser.add_argument('--batch_size', type=int, default=8,\n",
    "                      help='Number of images per batch.')\n",
    "  parser.add_argument('--minimum_segment_size', type=int, default=2,\n",
    "                      help='Minimum number of TF ops in a TRT engine.')\n",
    "  parser.add_argument('--num_iterations', type=int, default=2048,\n",
    "                      help='How many iterations(batches) to evaluate.'\n",
    "                      'If not supplied, the whole set will be evaluated.')\n",
    "  parser.add_argument('--display_every', type=int, default=100,\n",
    "                      help='Number of iterations executed between'\n",
    "                      'two consecutive display of metrics')\n",
    "  parser.add_argument('--use_synthetic', action='store_true',\n",
    "                      help='If set, one batch of random data is'\n",
    "                      'generated and used at every iteration.')\n",
    "  parser.add_argument('--num_warmup_iterations', type=int, default=50,\n",
    "                      help='Number of initial iterations skipped from timing')\n",
    "  parser.add_argument('--num_calib_inputs', type=int, default=500,\n",
    "                      help='Number of inputs (e.g. images) used for'\n",
    "                      'calibration (last batch is skipped in case'\n",
    "                      'it is not full)')\n",
    "  parser.add_argument('--gpu_mem_cap', type=int, default=0,\n",
    "                      help='Upper bound for GPU memory in MB.'\n",
    "                      'Default is 0 which means allow_growth will be used')\n",
    "  parser.add_argument('--max_workspace_size', type=int, default=(1<<30),\n",
    "                      help='workspace size in bytes')\n",
    "  parser.add_argument('--mode', choices=['validation', 'benchmark'],\n",
    "                      default='validation',\n",
    "                      help='Which mode to use (validation or benchmark)')\n",
    "  parser.add_argument('--target_duration', type=int, default=None,\n",
    "                      help='If set, script will run for specified'\n",
    "                      'number of seconds.')\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  if args.precision != 'FP32' and not args.use_trt:\n",
    "    raise ValueError('TensorRT must be enabled for FP16'\n",
    "                     'or INT8 modes (--use_trt).')\n",
    "  if (args.precision == 'INT8' and not args.calib_data_dir\n",
    "      and not args.use_synthetic):\n",
    "    raise ValueError('--calib_data_dir is required for INT8 mode')\n",
    "  if (args.num_iterations is not None\n",
    "      and args.num_iterations <= args.num_warmup_iterations):\n",
    "    raise ValueError(\n",
    "        '--num_iterations must be larger than --num_warmup_iterations '\n",
    "        '({} <= {})'.format(args.num_iterations, args.num_warmup_iterations))\n",
    "  if args.num_calib_inputs < args.batch_size:\n",
    "    raise ValueError(\n",
    "        '--num_calib_inputs must not be smaller than --batch_size'\n",
    "        '({} <= {})'.format(args.num_calib_inputs, args.batch_size))\n",
    "  if args.mode == 'validation' and args.use_synthetic:\n",
    "    raise ValueError('Cannot use both validation mode and synthetic dataset')\n",
    "  if args.data_dir is None and not args.use_synthetic:\n",
    "    raise ValueError(\"--data_dir required if you are not using synthetic data\")\n",
    "  if args.use_synthetic and args.num_iterations is None:\n",
    "    raise ValueError(\"--num_iterations is required for --use_synthetic\")\n",
    "  if args.use_trt and not args.output_saved_model_dir:\n",
    "    raise ValueError(\"--output_saved_model_dir must be set if use_trt=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_gpu_mem_cap = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4ef8b9f1f5e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig_gpu_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_mem_cap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "config_gpu_memory(args_gpu_mem_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_max_workspace_size = (1 << 31) \n",
    "args_precision = 'FP16'\n",
    "args_minimum_segment_size = 2\n",
    "args_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_trt_conversion_params(\n",
    "      args.max_workspace_size,\n",
    "      args.precision,\n",
    "      args.minimum_segment_size,\n",
    "      args.batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input_saved_model_dir = \"models/input\"\n",
    "args_input_saved_model_dir = \"models/output\"\n",
    "args_data_dir = \n",
    "args_calib_data_dir\n",
    "args_annotation_path\n",
    "args_input_size\n",
    "args_use_trt\n",
    "args_batch_size\n",
    "args_num_calib_inputs\n",
    "args_use_synthetic\n",
    "args_optimize_offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_func, times = get_graph_func(\n",
    "      input_saved_model_dir=args.input_saved_model_dir,\n",
    "      output_saved_model_dir=args.output_saved_model_dir,\n",
    "      data_dir=args.data_dir,\n",
    "      calib_data_dir=args.calib_data_dir,\n",
    "      annotation_path=args.annotation_path,\n",
    "      input_size=args.input_size,\n",
    "      conversion_params=params,\n",
    "      use_trt=args.use_trt,\n",
    "      batch_size=args.batch_size,\n",
    "      num_calib_inputs=args.num_calib_inputs,\n",
    "      use_synthetic=args.use_synthetic,\n",
    "      optimize_offline=args.optimize_offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(input_dict, prefix='  ', postfix=''):\n",
    "    for k, v in sorted(input_dict.items()):\n",
    "      print('{}{}: {}{}'.format(prefix, k, '%.1f'%v if isinstance(v, float) else v, postfix))\n",
    "  print('Benchmark arguments:')\n",
    "  print_dict(vars(args))\n",
    "  print('TensorRT Conversion Params:')\n",
    "  print_dict(dict(params._asdict()))\n",
    "  print('Conversion times:')\n",
    "  print_dict(times, postfix='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, predictions, image_ids = run_inference(graph_func,\n",
    "                data_dir=args.data_dir,\n",
    "                annotation_path=args.annotation_path,\n",
    "                batch_size=args.batch_size,\n",
    "                num_iterations=args.num_iterations,\n",
    "                num_warmup_iterations=args.num_warmup_iterations,\n",
    "                input_size=args.input_size,\n",
    "                use_synthetic=args.use_synthetic,\n",
    "                display_every=args.display_every,\n",
    "                mode=args.mode,\n",
    "                target_duration=args.target_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results:')\n",
    "  if args.mode == 'validation':\n",
    "    mAP = eval_model(predictions, image_ids, args.annotation_path)\n",
    "    print('  mAP: %f' % mAP)\n",
    "  print('  images/sec: %d' % results['images_per_sec'])\n",
    "  print('  99th_percentile(ms): %.2f' % results['99th_percentile'])\n",
    "  print('  total_time(s): %.1f' % results['total_time'])\n",
    "  print('  latency_mean(ms): %.2f' % results['latency_mean'])\n",
    "  print('  latency_median(ms): %.2f' % results['latency_median'])\n",
    "  print('  latency_min(ms): %.2f' % results['latency_min'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
